#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
COCOæ•°æ®é›†ä¸‹è½½è„šæœ¬ - æ”¯æŒæ–­ç‚¹ç»­ä¼ 

ä¸‹è½½COCO val2014æ•°æ®é›†å¹¶è§£å‹åˆ°æŒ‡å®šç›®å½•
"""

import os
import sys
import requests
import zipfile
from pathlib import Path
from tqdm import tqdm
import argparse


def download_with_resume(url, local_path, chunk_size=8192):
    """
    æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„ä¸‹è½½å‡½æ•°
    
    Args:
        url: ä¸‹è½½é“¾æ¥
        local_path: æœ¬åœ°ä¿å­˜è·¯å¾„
        chunk_size: ä¸‹è½½å—å¤§å°
    """
    local_path = Path(local_path)
    
    # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶
    resume_byte_pos = 0
    if local_path.exists():
        resume_byte_pos = local_path.stat().st_size
        print(f"å‘ç°å·²ä¸‹è½½æ–‡ä»¶ï¼Œä» {resume_byte_pos / 1024 / 1024:.1f} MB å¤„ç»§ç»­ä¸‹è½½")
    
    # è®¾ç½®è¯·æ±‚å¤´æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    headers = {}
    if resume_byte_pos > 0:
        headers['Range'] = f'bytes={resume_byte_pos}-'
    
    try:
        # å‘é€è¯·æ±‚
        response = requests.get(url, headers=headers, stream=True, timeout=30)
        
        # æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦æ”¯æŒæ–­ç‚¹ç»­ä¼ 
        if resume_byte_pos > 0 and response.status_code != 206:
            print("æœåŠ¡å™¨ä¸æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼Œé‡æ–°å¼€å§‹ä¸‹è½½")
            resume_byte_pos = 0
            response = requests.get(url, stream=True, timeout=30)
        
        response.raise_for_status()
        
        # è·å–æ–‡ä»¶æ€»å¤§å°
        if 'content-length' in response.headers:
            total_size = int(response.headers['content-length'])
            if resume_byte_pos > 0:
                total_size += resume_byte_pos
        else:
            total_size = None
        
        # åˆ›å»ºç›®å½•
        local_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¸‹è½½æ–‡ä»¶
        mode = 'ab' if resume_byte_pos > 0 else 'wb'
        with open(local_path, mode) as f:
            with tqdm(
                total=total_size,
                initial=resume_byte_pos,
                unit='B',
                unit_scale=True,
                desc=local_path.name
            ) as pbar:
                for chunk in response.iter_content(chunk_size=chunk_size):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
        
        print(f"âœ“ ä¸‹è½½å®Œæˆ: {local_path}")
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"ä¸‹è½½å¤±è´¥: {e}")
        return False
    except KeyboardInterrupt:
        print("\nä¸‹è½½è¢«ä¸­æ–­ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶å°†ä»æ–­ç‚¹ç»§ç»­")
        return False
    except Exception as e:
        print(f"ä¸‹è½½è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return False


def extract_zip(zip_path, extract_to):
    """
    è§£å‹ZIPæ–‡ä»¶
    
    Args:
        zip_path: ZIPæ–‡ä»¶è·¯å¾„
        extract_to: è§£å‹ç›®æ ‡ç›®å½•
    """
    zip_path = Path(zip_path)
    extract_to = Path(extract_to)
    
    if not zip_path.exists():
        print(f"ZIPæ–‡ä»¶ä¸å­˜åœ¨: {zip_path}")
        return False
    
    print(f"æ­£åœ¨è§£å‹ {zip_path.name} åˆ° {extract_to}")
    
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            # è·å–ZIPæ–‡ä»¶ä¸­çš„æ–‡ä»¶åˆ—è¡¨
            file_list = zip_ref.namelist()
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè§£å‹è¿›åº¦
            with tqdm(total=len(file_list), desc="è§£å‹æ–‡ä»¶") as pbar:
                for file_name in file_list:
                    zip_ref.extract(file_name, extract_to)
                    pbar.update(1)
        
        print(f"âœ“ è§£å‹å®Œæˆ: {extract_to}")
        return True
        
    except zipfile.BadZipFile:
        print(f"âŒ ZIPæ–‡ä»¶æŸå: {zip_path}")
        return False
    except Exception as e:
        print(f"è§£å‹å¤±è´¥: {e}")
        return False


def verify_download(file_path, expected_size=None):
    """
    éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        expected_size: æœŸæœ›çš„æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        return False
    
    file_size = file_path.stat().st_size
    print(f"æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.1f} MB")
    
    if expected_size and abs(file_size - expected_size) > 1024:  # å…è®¸1KBçš„è¯¯å·®
        print(f"âŒ æ–‡ä»¶å¤§å°ä¸åŒ¹é…ï¼ŒæœŸæœ›: {expected_size / 1024 / 1024:.1f} MB")
        return False
    
    # å°è¯•æ‰“å¼€ZIPæ–‡ä»¶éªŒè¯å®Œæ•´æ€§
    try:
        with zipfile.ZipFile(file_path, 'r') as zip_ref:
            # æµ‹è¯•ZIPæ–‡ä»¶å®Œæ•´æ€§
            bad_file = zip_ref.testzip()
            if bad_file:
                print(f"âŒ ZIPæ–‡ä»¶æŸåï¼ŒæŸåçš„æ–‡ä»¶: {bad_file}")
                return False
            else:
                print("âœ“ ZIPæ–‡ä»¶å®Œæ•´æ€§éªŒè¯é€šè¿‡")
                return True
    except zipfile.BadZipFile:
        print("âŒ ZIPæ–‡ä»¶æ ¼å¼é”™è¯¯")
        return False


def download_and_extract_dataset(dataset_name, url, output_dir, keep_zip=False, only_download=False, only_extract=False):
    """
    ä¸‹è½½å¹¶è§£å‹å•ä¸ªæ•°æ®é›†
    
    Args:
        dataset_name: æ•°æ®é›†åç§° (å¦‚ 'val2014')
        url: ä¸‹è½½é“¾æ¥
        output_dir: è¾“å‡ºç›®å½•
        keep_zip: æ˜¯å¦ä¿ç•™ZIPæ–‡ä»¶
        only_download: åªä¸‹è½½ä¸è§£å‹
        only_extract: åªè§£å‹å·²ä¸‹è½½çš„æ–‡ä»¶
    
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    output_dir = Path(output_dir)
    zip_path = output_dir / f"{dataset_name}.zip"
    
    print(f"\n{'='*20} {dataset_name.upper()} {'='*20}")
    print(f"ZIPæ–‡ä»¶: {zip_path}")
    
    # ä¸‹è½½æ–‡ä»¶
    if not only_extract:
        print(f"å¼€å§‹ä¸‹è½½ {dataset_name}.zip...")
        
        success = download_with_resume(url, zip_path)
        
        if not success:
            print(f"âŒ {dataset_name} ä¸‹è½½å¤±è´¥")
            return False
        
        # éªŒè¯ä¸‹è½½çš„æ–‡ä»¶
        print(f"éªŒè¯ {dataset_name} æ–‡ä»¶...")
        if not verify_download(zip_path):
            print(f"âŒ {dataset_name} æ–‡ä»¶éªŒè¯å¤±è´¥")
            return False
    
    # è§£å‹æ–‡ä»¶
    if not only_download:
        if zip_path.exists():
            print(f"å¼€å§‹è§£å‹ {dataset_name}...")
            success = extract_zip(zip_path, output_dir)
            
            if not success:
                print(f"âŒ {dataset_name} è§£å‹å¤±è´¥")
                return False
            
            # éªŒè¯è§£å‹ç»“æœ
            dataset_dir = output_dir / dataset_name
            if dataset_dir.exists():
                image_count = len(list(dataset_dir.glob("*.jpg")))
                print(f"âœ“ {dataset_name} è§£å‹å®Œæˆï¼Œå…± {image_count} å¼ å›¾ç‰‡")
            else:
                print(f"âŒ è§£å‹åæœªæ‰¾åˆ° {dataset_name} ç›®å½•")
                return False
            
            # åˆ é™¤ZIPæ–‡ä»¶ï¼ˆå¦‚æœä¸ä¿ç•™ï¼‰
            if not keep_zip:
                print(f"åˆ é™¤ZIPæ–‡ä»¶: {zip_path}")
                zip_path.unlink()
        else:
            print(f"âŒ ZIPæ–‡ä»¶ä¸å­˜åœ¨: {zip_path}")
            return False
    
    return True


def main():
    parser = argparse.ArgumentParser(description='ä¸‹è½½COCOæ•°æ®é›† (train2014, val2014, test2014)')
    parser.add_argument('--output-dir', type=str, 
                       default='/data/dlf/code/Field-Fidelity/data/coco',
                       help='è¾“å‡ºç›®å½•')
    parser.add_argument('--datasets', type=str, nargs='+',
                       default=['val2014', 'train2014','test2014'],
                       choices=['train2014', 'val2014', 'test2014'],
                       help='è¦ä¸‹è½½çš„æ•°æ®é›† (é»˜è®¤: val2014 train2014)')
    parser.add_argument('--keep-zip', action='store_true',
                       help='ä¿ç•™ZIPæ–‡ä»¶')
    parser.add_argument('--only-download', action='store_true',
                       help='åªä¸‹è½½ä¸è§£å‹')
    parser.add_argument('--only-extract', action='store_true',
                       help='åªè§£å‹å·²ä¸‹è½½çš„æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # COCOæ•°æ®é›†ä¸‹è½½é“¾æ¥
    dataset_urls = {
        'train2014': 'http://images.cocodataset.org/zips/train2014.zip',
        'val2014': 'http://images.cocodataset.org/zips/val2014.zip',
        'test2014': 'http://images.cocodataset.org/zips/test2014.zip'
    }
    
    # æ•°æ®é›†å¤§å°ä¿¡æ¯ (å¤§çº¦å€¼ï¼Œç”¨äºæ˜¾ç¤º)
    dataset_sizes = {
        'train2014': '13.0 GB (çº¦82,783å¼ å›¾ç‰‡)',
        'val2014': '6.2 GB (çº¦40,504å¼ å›¾ç‰‡)', 
        'test2014': '6.6 GB (çº¦40,775å¼ å›¾ç‰‡)'
    }
    
    output_dir = Path(args.output_dir)
    
    print("COCO æ•°æ®é›†ä¸‹è½½å·¥å…·")
    print("=" * 60)
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"è¦ä¸‹è½½çš„æ•°æ®é›†: {', '.join(args.datasets)}")
    
    # æ˜¾ç¤ºæ•°æ®é›†å¤§å°ä¿¡æ¯
    print("\næ•°æ®é›†å¤§å°ä¿¡æ¯:")
    for dataset in args.datasets:
        print(f"  {dataset}: {dataset_sizes[dataset]}")
    
    total_size_gb = sum([13.0 if d == 'train2014' else 6.2 if d == 'val2014' else 6.6 for d in args.datasets])
    print(f"\né¢„è®¡æ€»ä¸‹è½½å¤§å°: ~{total_size_gb:.1f} GB")
    
    # ç¡®è®¤ç»§ç»­
    if not args.only_extract:
        response = input("\næ˜¯å¦ç»§ç»­ä¸‹è½½? (y/N): ")
        if response.lower() not in ['y', 'yes']:
            print("ä¸‹è½½å·²å–æ¶ˆ")
            return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¸‹è½½å’Œè§£å‹æ¯ä¸ªæ•°æ®é›†
    success_count = 0
    failed_datasets = []
    
    for dataset_name in args.datasets:
        url = dataset_urls[dataset_name]
        
        try:
            success = download_and_extract_dataset(
                dataset_name=dataset_name,
                url=url,
                output_dir=output_dir,
                keep_zip=args.keep_zip,
                only_download=args.only_download,
                only_extract=args.only_extract
            )
            
            if success:
                success_count += 1
            else:
                failed_datasets.append(dataset_name)
                
        except KeyboardInterrupt:
            print(f"\nä¸‹è½½è¢«ä¸­æ–­ï¼Œ{dataset_name} æœªå®Œæˆ")
            print("ä¸‹æ¬¡è¿è¡Œæ—¶å°†ä»æ–­ç‚¹ç»§ç»­")
            break
        except Exception as e:
            print(f"âŒ {dataset_name} å¤„ç†å¤±è´¥: {e}")
            failed_datasets.append(dataset_name)
    
    # æ€»ç»“
    print(f"\n{'='*60}")
    print(f"ä¸‹è½½å®Œæˆæ€»ç»“:")
    print(f"æˆåŠŸ: {success_count}/{len(args.datasets)} ä¸ªæ•°æ®é›†")
    
    if failed_datasets:
        print(f"å¤±è´¥: {', '.join(failed_datasets)}")
    
    # éªŒè¯æœ€ç»ˆç»“æœ
    if success_count > 0:
        print(f"\néªŒè¯ä¸‹è½½ç»“æœ...")
        for dataset_name in args.datasets:
            if dataset_name not in failed_datasets:
                dataset_dir = output_dir / dataset_name
                if dataset_dir.exists():
                    image_count = len(list(dataset_dir.glob("*.jpg")))
                    print(f"âœ“ {dataset_name}: {image_count} å¼ å›¾ç‰‡")
        
        # æ£€æŸ¥IDKæ•°æ®é›†éœ€è¦çš„ç‰¹å®šå›¾ç‰‡
        test_images = [
            "val2014/COCO_val2014_000000262162.jpg",
            "val2014/COCO_val2014_000000131108.jpg"
        ]
        
        print(f"\næ£€æŸ¥IDKæ•°æ®é›†éœ€è¦çš„æµ‹è¯•å›¾ç‰‡:")
        for img_path in test_images:
            full_path = output_dir / img_path
            if full_path.exists():
                print(f"âœ“ {img_path}")
            else:
                print(f"âŒ {img_path}")
        
        print(f"\nâœ… COCOæ•°æ®é›†å‡†å¤‡å®Œæˆ!")
        print(f"æ•°æ®ç›®å½•: {output_dir}")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡æç¤º
        print(f"\nğŸ’¡ å»ºè®®è®¾ç½®ç¯å¢ƒå˜é‡:")
        print(f"export COCO_ROOT=\"{output_dir}\"")
        
    else:
        print(f"\nâŒ æ‰€æœ‰æ•°æ®é›†ä¸‹è½½å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
